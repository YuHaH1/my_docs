---
lang: zh-CN
title: 项目中得到的问题
description: 关键功能
collapsible: true
---
# 项目中得到的问题

## 1.埋点API多平台调用

描述：当用户点击时触发一个埋点方法，发送多个平台，常规方法怎么做？如下`logerA.send()`,`logerB.send()`，但是页面可能很多地方都会调用，每次都写还是很恶心的，用proxy代理

~~~js
const arr = [
    {
        send(val) {
            console.log(val)
        }
    },
    {
        send(val) {
            console.log(val)
        }
    },
]
const obj = new Proxy(arr, {
    get(target, key, receiver) {
        return function () {
            target.forEach(item => {
                item[key] && item[key].call(receiver,...arguments)
            })
        }
    }
})
obj.send(val)
~~~

这样只需要`obj.send`就可以直接调用数组里的`send API`而且如果假设还有其他平台的加入，直接在数组中继续加，页面中的其他使用了的就自动调用，这就是实现了解耦。



## 2.高度自动过度

~~~js
const btn = document.querySelector('.btn')
const content = document.querySelector('.detail')

btn.onmouseenter = function(){
    content.style.height = 'auto'
    const {height} = content.getBoundingClientRect()
    content.style.height = 0
    content.offsetHeight  //强制渲染
    content.style.height = height
}
btn.onmouseleave = function(){
    content.style.height = 0;
    content.style.transition = '.3s'
}
~~~



## 3.SEO

### 预渲染

Vue项目`@dreysolano/prerender-spa-plugin`

~~~js
plugins: [
    new PrerenderSPAPlugin({
     // 生成文件的路径，要与webpakc打包输出的路径的一致。
     // 这个目录只能有一级，如果目录层次大于一级，在生成的时候不会有任何错误提示，在预渲染的时候会卡着不动。
     staticDir: path.join(__dirname,'dist'),
     // 对应自己的路由文件，比如a有参数，就需要写成 /a/param1。
     routes: ['/', '/product','/about'],
     // 这个很重要，如果没有配置这段，也不会进行预编译
     minify: {
        minifyCSS: true, // css压缩
        removeComments: true // 移除注释
     },
     renderer: new Renderer({
      	// __PRERENDER_INJECTED这个属性名会添加到window对象上，作为inject的内容
      	injectProperty: '__PRERENDER_INJECTED',
      	inject: {
       		foo: 'bar'
      	},
      	headless: false,//渲染时显示到浏览器窗口用于调试
      	// 在 main.js 中 document.dispatchEvent(new Event('custom-render-trigger'))，两者的事件名称要对应上。
      	renderAfterDocumentEvent: 'custom-render-trigger',
      	//等到document.querSelector('my-app-element')检查到该元素开始渲染
      	renderAfterElementExists:'my-app-element'
     })
    }),
   ],
~~~



原理，`prerender-spa-plugin` 利用了 `Puppeteer`的爬取页面的功能。 `Puppeteer` 是一个 `Chrome`官方出品的 `headlessChromenode` 库。它提供了一系列的 API, 可以在无 UI 的情况下调用 `Chrome` 的功能, 适用于爬虫、自动化处理等各种场景。它很强大，所以很简单就能将运行时的 `HTML` 打包到文件中。原理是在 `Webpack` 构建阶段的最后，在本地启动一个 `Puppeteer` 的服务，访问配置了预渲染的路由，预渲染插件会创建一个虚拟的浏览器环境，然后使用 Vue 的 `renderToString` 方法将组件渲染为字符串形式的 HTML,然后将 `Puppeteer` 中渲染的页面输出到 `HTML` 文件中，并建立路由对应的目录。使用预渲染的方式生成静态 HTML 文件后，你的应用在浏览器中加载时将不再是典型的单页面应用（SPA）。相反，每个路由路径都会有对应的独立的 HTML 文件。

### META标签配置

meta标签的`属性`有两种：`name`和`http- equiv`。

 **"name"属性有以下配置项：**

* Keywords(关键词，现在不再重要了)：逗号分隔的关键词列表（告诉搜索引擎页面是与什么相关的）；
* description(网站内容描述，很重要)：页面描述。搜索引擎会把这个描述显示在搜索结果中；
* format-detection：格式检测，比如禁止识别电话，邮箱等；
* author：作者的名字；
* Robots：用来告诉搜索机器人哪些页面需要索引，哪些页面不需要索引；
* theme-color：网站主题色；

```js
<meta name="keywords" content="掘金,稀土,Vue.js,前端面试题,Kotlin,ReactNative,Python">

<meta name="description" content="掘金是面向全球中文开发者的技术内容分享与交流平台。我们通过技术文章、沸点、课程、直播等产品和服务，打造一个激发开发者创作灵感，激励开发者沉淀分享，陪伴开发者成长的综合类技术社区。">

<meta name="format-detection" content="telephone=no">

<meta name="author" content="cece">

<Meta name="Robots" Content="Nofollow">
/** 
all：文件将被检索，且页面上的链接可以被查询；  
none：文件将不被检索，且页面上的链接不可以被查询；(和 "noindex, no follow" 起相同作用)  
index：文件将被检索；（让robot/spider登录）  
follow：页面上的链接可以被查询；  
noindex：文件将不被检索，但页面上的链接可以被查询；(不让robot/spider登录)  
nofollow：文件将不被检索，页面上的链接可以被查询。(不让robot/spider顺着此页的连接往下探找)
*/

<meta name="theme-color" content="#4285f4" />
```

（一） `keywords`

上面我们也提到了，现在 `keywords` 关键词已经被各大搜索引擎降低了权重，所以可以设置也可以不设置，但我认为它仍然有他存在的价值。

如果你决定配置网站关键词，需要注意以下几点：

* keywords 关键词数量控制在1-4个左右，避免关键词堆砌；
* 合理选择长尾关键词（长尾关键词一般是2-3个词组成。例如，目标关键词是服装，其长尾关键词可以是男士服装、冬装等），长尾关键词虽然相对核心关键词的搜索量小很多，但是它带来的流量精准度非常高，后期的转化效果更好；
* 避免使用过于专业的词汇。过于专业的词汇的搜索量较低；
* 减少使用热门关键词，要选择合适的关键词（搜索量大、转化率高、定位精准）。

（二）`Description`

* `Description`（页面描述）的长度最好控制在120~200个字符；
* `Description`要让用户知道将从页面中获得什么；
* 在`Description`中合理使用行动号召（CTA）用语（例如“了解更多”、“立即获取”、“免费试用”等等……）；
* `Description`应该包含页面的核心关键字；
* 为每个页面创建独一无二的`Description`；

HTML语义化



### robots文件

蜘蛛在访问一个网站时，会首先会检查该网站的根域下是否有一个叫做 robots.txt的纯文本文件，这个文件用于指定spider在您网站上的抓取范围。

如果你有哪些页面不想被蜘蛛访问，则可以通过robots文件告诉蜘蛛不想被搜索引擎收录的部分或者指定搜索引擎只收录特定的部分。

robots文件内容语法：
 此文件主要由两种键值对组成：

1. **User-agent:**  该项的值用于描述搜索引擎蜘蛛的名字。如果该项的值设为*，则该协议对任何机器人均有效。
2. **Disallow:**  该项的值用于描述不希望被访问到的一个URL，一个目录或者整个网站。以Disallow 开头的URL 均不会被搜索引擎蜘蛛访问到。任何一条Disallow 记录为空，说明该网站的所有部分都允许被访问。

掘金的robots.txt

~~~txt
User-agent: *

Disallow: /subscribe/subscribed
Disallow: /user/settings
Disallow: /drafts
Disallow: /news-drafts
Disallow: /editor
Disallow: /oauth-result
Disallow: /search
Disallow: /equation
Disallow: /book/order
Disallow: /books/payment
Disallow: /appview
Disallow: /creator
Disallow: /notification
Disallow: /translate
Disallow: /zhuanlan
Disallow: /spost

Sitemap: https://juejin.cn/sitemap/posts/index.xml
Sitemap: https://juejin.cn/sitemap/user/index.xml
Sitemap: https://juejin.cn/sitemap/news/index.xml
Sitemap: https://juejin.cn/sitemap/columns/index.xml
Sitemap: https://juejin.cn/sitemap/tag/index.xml
Sitemap: https://juejin.cn/sitemap/pin/index.xml
Sitemap: https://juejin.cn/sitemap/collections/index.xml
Sitemap: https://juejin.cn/sitemap/keywords/index.xml
Sitemap: https://juejin.cn/sitemap/user_posts/index.xml
Sitemap: https://juejin.cn/sitemap/user_pins/index.xml

/*
*网站目录下所有文件均能被所有搜索引擎蜘蛛访问*
User-agent: *
Disallow:

*禁止所有搜索引擎蜘蛛访问网站的任何部分*
User-agent: *
Disallow: /

*禁止所有的搜索引擎蜘蛛访问网站的几个目录*
User-agent: *
Disallow: /a/
Disallow: /b/

*只允许某个搜索引擎蜘蛛访问*
User-agent: Googlebot
Disallow: 
*/
~~~

### sitemap站点地图

Sitemap，即站点地图，它是一个网站的全部URL列表，同时可以列出每个网址的其他元数据（上次更新的时间、更改的频率以及相对于网站上其他网址的重要程度为何等）。它可以为搜索引擎的蜘蛛进行导航，更快的找到全站中的所有链接，更全面的获取网站信息。为了保证链接的全面性和准确性，应该自动不定期更新sitemap站点地图。

一般网站的sitemap文件都会有以下两种格式：

sitemap.xml，这是大部分搜索引擎所使用的用于提交网站网址的XML文件；
 sitemap.html，这是可直接放在网站上用于用户访问或搜索引擎快速找到全站链接的页面（每页最多500条，自动分页）；

sitemap.xml 文件内容格式大致如下：

```xml
<?xml version="1.0" encoding="UTF-8"?>
    <urlset
        xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"
        xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
        xsi:schemaLocation="http://www.sitemaps.org/schemas/sitemap/0.9
           http://www.sitemaps.org/schemas/sitemap/0.9/sitemap.xsd"
    >
    <url>
     <loc>https://www.cece.com/</loc>
     <priority>0.3</priority>
     <lastmod>2023-05-17</lastmod>
     <changefreq>weekly</changefreq>
    </url>
    </urlset>
```

网上有很多生成sitemap文件的站长工具，例如：
 [sitemap.zhetao.com/](https://link.juejin.cn?target=https%3A%2F%2Fsitemap.zhetao.com%2F)
 [tools.bugscaner.com/sitemapspid…](https://link.juejin.cn?target=http%3A%2F%2Ftools.bugscaner.com%2Fsitemapspider)

生成的sitemap文件一般放在项目根目录下，然后可以在各个搜索引擎的站点平台提交sitemap.xml文件。

### 内链和外链

尤其是外链的建设对权重影响大。 先来区分下网站内链和外链：

**内链**：从自己网站的一个页面指向另外一个页面。通过内链让网站内部形成网状结构，让蜘蛛的广度和深度达到最大化。

**外链**：在别的网站导入自己网站的链接。通过外链提升网站权重，提高网站流量。外链有以下几个好处：

* 提升网站权重
* 能够吸引蜘蛛来抓取网站
* 提升关键词排名
* 提升网址或品牌的曝光度
* 给网站带来流量

外链能够为我们的网站带来流量，所以外链数量越多越好是必然的。但是，**一定要注意外链的质量**，例如对方网站没有被搜索引擎收录，对方网站性能过差，死链等，这些低质量的外链反而会影响到本站的排名。

另外，在添加内链外链的过程中，要注意在 a 标签中对 `nofollow` 和·`external` 属性的使用。

~~~js
//带有rel=nofollow 属性的链接会告诉搜索引擎忽略这个链接。阻止搜索引擎对该页面进行追踪。从而避免权重分散。这个属性只对搜索引擎有效，这是一个纯粹的SEO优化标签。
//使用场景：
	//屏蔽一些垃圾链接，比如网站上面评论里面的站外链接，论坛里面用户留下的链接等；
	//外链的内容与本站无关时，建议使用nofollow；
	//外链站点不稳定，性能较差时，建议使用nofollow；
	//友链一般作为网站交换链接，互惠互利，是不会设置nofollow的，所以在交换友链之前，一定要对对方的网站质量进行审核；
	//内部链接密度过大，页面重要性不高时，可以使用nofoolw，例如很多网站常有的“关于我们”页面，比如掘金的：
<a rel="nofollow" href="http://www.baidu.com/">百度</a>

external字面意思是“外部的”，a 标签加上这个属性代表这个链接是外部链接，非本站链接，点击时会在新窗口中打开，它和target="_blank"效果一样。external 可以告诉搜索引擎这是一个外部链接，非本站的链接。
<a rel="external" href="http://www.baidu.com/">百度</a>
~~~

### 网址规范

什么是网址规范？简单举个例子：
 [juejin.cn](https://juejin.cn)
 [www.juejin.cn](https://www.juejin.cn)
 [www.juejin.cn/index.html](https://www.juejin.cn/index.html)
 这几个网址虽然url不同，搜索引擎也确实把他当作不同的网址，但是其实这些网址返回的都是同一个页面，这就是不规范网址。

**网页规范化的两个好处：**

* 解决网站由于网站url链接不一样，但网页内容是一样而造成搜索引擎重复收录的问题；
* 有利于URL权重集中。

**解决方法：**
 在页面的`head` 标签中，加入以下canonical标签，指定规范化网址。

```bash
bash复制代码<head>
  <link rel="canonical" href="href="https://juejin.cn"/>
</head>
```

例如掘金的规范化网址：`<link data-n-head="ssr" rel="canonical" href="https://juejin.cn">`

虽然`canonical标签`可以规范化网址，但是以下四种情况必须配置301重定向：

1. 网站替换域名后，通过301永久重定向旧域名重定向到新域名，弥补流量损失和SEO;
2. 如果删除掉网站中的一些页面，但是这个页面有一定的流量和权重，可以利用301重定向到合适的页面避免流量流失；
3. 网站改版或因为其他原因导致页面地址发生变化，为避免出现死链接，可以通过 301 重定向来解决；
4. 如果您有多个空闲的域名需要指向相同的站点，则可以使用301永久重定向；

### 301重定向

301/302重定向是 SEO优化中一种重要的自动转向技术。301重定向是当搜索引擎向网站服务器发出访问请求时，服务返回的HTTP数据流中头信息(header)部分状态码的一种，表示本网址永久性转移到另一个地址。302重定向则表示暂时转移。

301重定向与上一点所说的网址规范化有着类似的作用，与此同时，它还具有以下作用：

* 集中域名权重，301 网址跳转其实是对域名权重进行转移，比如 `www.juejin.cn` 重定向到 `juejin.cn`，其实是把 `www.juejin.cn` 的权重转移到了 `juejin.cn`，从而增加 `juejin.cn` 域名的权重；
* 避免重复收录；
* 网页PR（PageRank-网页级别）是用来评估一个网站页面相对于网站其他页面重要性的一个算法，301定向跳转有利于网站PR的传递；
* 优化用户体验，网址规范化可以让用户更好地记住我们的网站，可以将域名统一重定向到某一个域名，增加网站的记忆度，获取更好的用户体验。



<CommentService/>